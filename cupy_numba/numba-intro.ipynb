{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ba44776-ef27-4684-9ce4-1a34ce2e3d57",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Notebook 3: Introduction to Numba**\n",
    "\n",
    "This tutorial introduces Numba basics and common usages before diving into our main nearest neighbors example. Numba is a just-in-time compiler optimized for numerically-focused Python functions that can work on the CPU or the GPU. Numba can compile a large number of NumPy functions and can create ufuncs (universal functions), parallelize loops, and generate GPU-accelerated code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24a3ca0-70a1-49f5-9880-836e0216b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import (cuda,  \n",
    "                   float32,\n",
    "                   jit, \n",
    "                   vectorize,\n",
    "                   guvectorize)\n",
    "\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb4665a-ad8c-4033-9169-d1912cb1f2e1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Numba Vectorize/Guvectorize**\n",
    "\n",
    "The Numba `vectorize` decorator compiles a Python function that takes a scalar input into a [NumPy ufuncs](https://numpy.org/doc/stable/reference/ufuncs.html). These ufuncs or universal functions work elementwise over NumPy ndarrays. This is an example of a dynamic unfunc (DUFunc), as we are not passing in any signatures to our vectorize decorator. When a different input type is passed, we will dynamically compile a new kernel.\n",
    "\n",
    "We are running the relative difference ufunc below on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b1682a-8177-4736-8497-df96faabd588",
   "metadata": {},
   "outputs": [],
   "source": [
    "@vectorize()\n",
    "def rel_diff_cpu(x, y):\n",
    "    return 2 * (x - y) / (x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4adb740-eb53-4144-b0bf-6feaaa0359fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "In the next cell we run our relative difference function on increasing NumPy array sizes. We are timing a NumPy ufunc implementation and our Numba ufunc implementation. Note we are ignoring divide by zero errors we might get with this method of random number generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2eaf45-0004-4f3d-8aee-3361181d3201",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_list = [10, 10000, 100000, 1000000, 10000000, 100000000]\n",
    "numpy_cpu_times = []\n",
    "numba_cpu_times = []\n",
    "\n",
    "np.seterr(divide='ignore')\n",
    "\n",
    "for size in size_list:\n",
    "    x=np.random.randn(size).astype(np.float32)\n",
    "    y=np.random.randn(size).astype(np.float32)\n",
    "\n",
    "    start_time_numpy = time.monotonic()\n",
    "    2 * (x - y) / (x + y)\n",
    "    numpy_cpu_times += [(time.monotonic() - start_time_numpy)]\n",
    "\n",
    "    start_time_numba_cpu = time.monotonic()\n",
    "    rel_diff_cpu(x, y)\n",
    "    numba_cpu_times += [(time.monotonic() - start_time_numba_cpu)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aab071-4baa-4f1f-89d7-01f90383be30",
   "metadata": {},
   "source": [
    "Next, lets plot these timings to show how we see a bigger performance benefit as we move to larger input arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1210e20b-f3c4-498b-8a55-81fc4ee6b8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(size_list, numba_cpu_times, label=\"Numba CPU\")\n",
    "plt.plot(size_list, numpy_cpu_times, label=\"NumPy\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Array size\")\n",
    "plt.ylabel(\"Time elapsed\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3c56d9-a450-4070-91cc-213f6baa10b1",
   "metadata": {},
   "source": [
    "Next let's walk through a `guvectorize` example, calculating a moving average. In this case we are illustrating Eager/decoration-time compilation where we pass the type signatures. In general, with `guvectorize` we can work on an arbitrary number of elements of input arrays (as opposed to one element at a time with `vectorize`).\n",
    "\n",
    "The layout in symbolic form `(n),()->(n)`is how we tell NumPy that we take in an input array of shape `n` and a constant which is represented by the `()`. Then the output of this function will also be of the same shape as our original input array `(n)`.\n",
    "\n",
    "Also note in guvectorize we don't have a return, but write our results to one of the arguments, in this case `out`. This is particularly useful because with guvectorize we can have return arrays of differing dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d9fd71-2d1e-4a99-a57a-6c1ba03e863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@guvectorize(['void(float64[:], intp[:], float64[:])'], '(n),()->(n)')\n",
    "def move_mean(a, window_arr, out):\n",
    "    window_width = window_arr[0]\n",
    "    asum = 0.0\n",
    "    count = 0\n",
    "    for i in range(window_width):\n",
    "        asum += a[i]\n",
    "        count += 1\n",
    "        out[i] = asum / count\n",
    "    for i in range(window_width, len(a)):\n",
    "        asum += a[i] - a[i - window_width]\n",
    "        out[i] = asum / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d553c621-e900-46f6-901d-26246e040fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.arange(30, dtype=np.float64).reshape(3,10)\n",
    "print(\"Input array:\")\n",
    "print(arr)\n",
    "print(\"Output array:\")\n",
    "print(move_mean(arr, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2549ecff-6194-423d-86c6-3577a5d92ae4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Custom Numba Kernels: Interoperability**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746fa39d-9742-4241-a68e-b615a097ba8c",
   "metadata": {},
   "source": [
    "In general, we recommend using CuPy arrays when writing Numba code targeting the GPU. Below in our L2 norm calculation we will use CuPy arrays with our Numba kernels.\n",
    "\n",
    "Our first `sum_reduce` function uses the `reduce` decorator which takes our binary summation operation and turns it into a reduction kernel, running one the GPU as indicated by the `cuda` in our decorator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e287d7d9-7209-491e-9fff-aad83cfef206",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.reduce\n",
    "def sum_reduce(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938e2eee-3fe1-4193-9330-9cba48d11596",
   "metadata": {},
   "source": [
    "The next kernel is our first just-in-time kernel targeting the GPU. We use:\n",
    "\n",
    "`cuda.grid` The absolute position of the current thread in the entire grid of blocks \\\n",
    "`cuda.gridsize` The size in threads of the entire grid of blocks\n",
    "\n",
    "We can safely loop over the data using one thread per data element, without assuming we have enough threads to cover the whole array size. Instead we loop over our data one grid size at a time (aka one stride)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c959f6-484f-4ebc-9604-ba56c21ea427",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def numba_l2_norm(x):\n",
    "    start = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "\n",
    "    for i in range(start, x.shape[0], stride):\n",
    "        x[i] = x[i] * x[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaefc23-f17d-4ed8-879b-a885c575d5a1",
   "metadata": {},
   "source": [
    "Go ahead and change our problem size `x` to see how varying the array size affects both NumPy and Numba performance. Also explore how we can adjust our  <code>threads_per_block</code> to get the best utilization of the GPU.\n",
    "\n",
    "Note, if we weren't comparing the NumPy implementation, we could create our d_x directly using <code>d_x = cp.random.rand(2**26)</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf2fbcb-e03b-46aa-a67d-e5835b806818",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(2**22)\n",
    "d_x =cp.array(x)\n",
    "threads_per_block = 1024\n",
    "blocks_per_grid = (d_x.size + (threads_per_block - 1)) // threads_per_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ec8a0c-deec-4dd5-b710-aff3d0dbb07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.monotonic()\n",
    "numba_l2_norm[blocks_per_grid, threads_per_block](d_x)\n",
    "output = cp.sqrt(sum_reduce(d_x))\n",
    "print(\"Numba time elapsed (fractional seconds): \", time.monotonic() - start_time)\n",
    "print(\"Numba output: \", output)\n",
    "\n",
    "start_time_np = time.monotonic()\n",
    "np.linalg.norm(x, 1)\n",
    "print(\"NumPy time elapsed (fractional seconds):\" , time.monotonic() - start_time_np)\n",
    "print(\"NumPy output: \", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0f4a20-496f-452f-8548-b682d0e39649",
   "metadata": {},
   "source": [
    "You will see the first couple of calls to the Numba kernel are slower than the NumPy implementation, but after a couple reruns, the performance levels out and Numba is much faster.\n",
    "\n",
    "This example also shows us how the GPU and specifically custom Numba kernels as we have written will perform well with very large input data (it also does better when the problem is more complicated than we have demonstrated above). To show this point, when you decrease the input size, the NumPy implementation will greatly outperform our solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1602d31e-74d0-47e5-84ca-7e61b2243bf6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Numba custom kernels: SAXPY**\n",
    "\n",
    "Below we have a simple SAXPY kernel that utilizes the `vectorize` decorator. We specify the expected types therefore doing eager compilation, and make sure to set the target to run on the GPU. You can see when we are calling a function with the decorator it is different from how we call our <code>@cuda.jit</code> functions. We do not have to pass our <code>[blocks_per_grid, threads_per_block]</code>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aff03c6-31c1-4a08-9458-47a566e4b775",
   "metadata": {},
   "outputs": [],
   "source": [
    "@vectorize(['float32(float32, float32, float32)'], target='cuda')\n",
    "def saxpy(a, x, y):\n",
    "    return a * x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f13a976-1d1c-47f0-95d5-84655d9ebf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2**25\n",
    "A=cp.random.randn(N).astype(cp.float32)\n",
    "B=cp.random.randn(N).astype(cp.float32)\n",
    "\n",
    "%time C = saxpy(2.0, A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e2981e-6ffe-4670-8f83-e5c371968e90",
   "metadata": {},
   "source": [
    "Experiment with differnt array sizes by changing `N` and seeing how the performance of our Numba timing changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aa9223-7aa9-42f8-9844-dc5c5d1d3ce7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Numba custom kernels: fast matrix multiplication**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fce2277-9862-4ed5-85a8-6ff34d6fd7da",
   "metadata": {},
   "source": [
    "In this final section of our Numba introduction we will use a common example of GPU performance - matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53a2174-a474-423e-bd31-aae382245213",
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_per_block = 16\n",
    "blocks_per_grid = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f267578-dbbf-4cdd-b08d-8ccd424fcf15",
   "metadata": {},
   "source": [
    "If you are new to CUDA concepts or thread hierarchy, keep the threads per block and BPG blocks per grid the same, but if you are curious, test out how changing that value affects your performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3e7d2c-35fe-4aca-a8d6-7abe582d5106",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def fast_matmul(A, B, C):\n",
    "    sA = cuda.shared.array(shape=(threads_per_block, threads_per_block), dtype=float32)\n",
    "    sB = cuda.shared.array(shape=(threads_per_block, threads_per_block), dtype=float32)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    blocks_per_grid = cuda.gridDim.x\n",
    "\n",
    "    tmp = float32(0.)\n",
    "    for i in range(blocks_per_grid):\n",
    "        sA[ty, tx] = 0\n",
    "        sB[ty, tx] = 0\n",
    "        if y < A.shape[0] and (tx + i * threads_per_block) < A.shape[1]:\n",
    "            sA[ty, tx] = A[y, tx + i * threads_per_block]\n",
    "        if x < B.shape[1] and (ty + i * threads_per_block) < B.shape[0]:\n",
    "            sB[ty, tx] = B[ty + i * threads_per_block, x]\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        for j in range(threads_per_block):\n",
    "            tmp += sA[ty, j] * sB[j, tx]\n",
    "            \n",
    "        cuda.syncthreads()\n",
    "    if y < C.shape[0] and x < C.shape[1]:\n",
    "        C[y, x] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f1a1f6-ccc6-46bc-8894-8b31218f16a7",
   "metadata": {},
   "source": [
    "Now we will show how to call and time this fast matrix multiplication kernel we have written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd5eeaf-6e1b-4430-9c35-aabd00044171",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_d = cp.arange(1000).reshape([50,20])\n",
    "y_d = cp.ones([50,20])\n",
    "z_d = cp.zeros([50,20])\n",
    "\n",
    "%time fast_matmul[blocks_per_grid, threads_per_block](x_d, y_d, z_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240dbdd3-224b-4980-bc40-d347077a1a81",
   "metadata": {},
   "source": [
    "Below we call our Numba kernel on NumPy arrays, see on the first pass we should get a warning about the conversion to a device array happening. The warning will go go away once you have run the cells again. Go ahead and run the cell above and below again, notice the changes in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ec5bf3-71ea-443a-a6be-30327d033dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_d = np.arange(1000).reshape([50,20])\n",
    "y_d = np.ones([50,20])\n",
    "z_d = np.zeros([50,20])\n",
    "\n",
    "%time fast_matmul[blocks_per_grid, threads_per_block](x_d, y_d, z_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dd0b27-a10b-4c80-a0ae-883d9dd6749c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Please restart the kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44058450-b452-40eb-9791-c5c2096a6c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
